
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Project Page</title>
    <!-- <link rel="stylesheet" href="styles.css"> -->
    <style>

        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 0;
            background-color: #f4f4f4;
            }

        header.title-section {
            background: #1d93e894;
            color: #ffffff;
            padding: 20px;
            /* border-bottom: 4px solid #d64545b0; */
            text-align: center;
            width: 100%;
        }


        header.title-section .authors-info {
            margin-top: 10px;
            color: #ffffff;
        }

        header.title-section .authors-info .authors {
            font-weight: bold;
            font-size: 1.2em;
            margin: 5px 0;
        }

        header.title-section .authors-info .affiliations {
            font-style: italic;
            font-size: 1em;
            margin: 5px 0;
        }

        header.title-section .authors-info sup {
            font-size: 0.8em;
            vertical-align: super;
        }

        @keyframes blink {
            0% { opacity: 1; }
            50% { opacity: 0; }
            100% { opacity: 1; }
        }

        .headline {
            text-align: center;
            font-size: 24px;
            font-weight: bold;
            color: red;
            animation: blink 1s infinite;
            margin: 20px 0;
        }

        nav.navigation {
            background: #50b3a2;
            padding: 10px 0;
            border-bottom: 1px solid #e0e0e0;
            width: 103%;
        }

        nav.navigation ul {
            list-style: none;
            padding: 0;
            margin: 0;
            display: flex;
            justify-content: center;
        }

        nav.navigation ul li {
            margin: 0 15px;
        }

        nav.navigation ul li a {
            color: #ffffff;
            text-decoration: none;
            font-weight: bold;
        }

        nav.navigation ul li a:hover {
            text-decoration: underline;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
        }

        section {
            margin: 20px 0;
            padding: 20px;
            background: #ffffff;
            border-radius: 8px;
            box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
        }

        section h2 {
            border-bottom: 2px solid #e8491d;
            padding-bottom: 10px;
        }

        a {
            color: #50b3a2;
            text-decoration: none;
        }

        a:hover {
            text-decoration: underline;
        }

        .image-container {
            margin-top: 20px;
            text-align: center;
        }

        .image-container img {
            max-width: 100%;
            height: auto;
            /* border: 2px solid #e8491d; */
            border-radius: 8px;
        }

        /* Styling for the Abstract section */
        #abstract {
            font-family: 'Georgia', serif; /* Change to your preferred font family */
            color: #555555; /* Lighter text color */
            font-weight: 300; /* Lighter font weight */
        }

        /* Citation section */
        #citation{
            text-align: left;
            background: #f5f5f5;
            padding: 15px;
            border: 1px solid #e0e0e0;
            border-radius: 10px;
            font-family: 'Courier New', Courier, monospace;
            max-width: 600px;
        }

        #citation h2 {
            font-size: 20px;
            margin-bottom: 10px;
        }

        .citation-content {
            font-size: 14px;
            color: #333;
            text-align: left;
        }

        .citation-note {
            color: #555555; /* Change text color to grey */
            font-family: 'Georgia', serif; /* Change font family to Georgia */
            font-weight: 400; /* Increase font weight */
        }

        .citation-bibtex {
            font-family: 'Courier New', monospace;
            background-color: #eee;
            padding: 10px;
            border-radius: 5px;
            display: block;
            text-align: left;
            margin: 0; /* Center the box */
            width: auto; /* Shrink the box to fit content */
            max-width: 100%; /* Ensure it doesn't overflow the container */
            white-space: pre-line;
        }
        /* Flexbox styling for side-by-side sections */
        .side-by-side {
            display: flex;
            gap: 20px;
        }

        #download, #code, #dataset {
            flex: 1;
        }

        .download-image-container {
            text-align: center;
            margin-top: 10px;
        }

        .download-image-container img {
            width: 250px; /* Set the desired width */
            height: auto; /* Maintain aspect ratio */
            border: 4px solid #1dcde800;
            border-radius: 4px;
        }

        /* Styling for GitHub icon */
        .icon {
            width: 250px;
            height: auto;
            vertical-align: middle;
            margin-right: 8px;
        }

        #resources a, #dataset a {
            display: flex;
            align-items: center;
            justify-content: center;
            text-align: center;
        }
        #dataset a {
            display: flex;
            align-items: left;
            justify-content: left;
            text-align: left
        }

        /* #contact p { */
           /* font-family: 'Georgia', serif; Change to your preferred font family */
            /* color: #555555; Lighter text color */
            /* font-weight: 300; Lighter font weight */
        /* } */
        #contact p {
            text-align: center;
            font-family: 'Georgia', serif;
            color: #555555; 
            font-weight: 700
        }

        #contact img {
            width: 100px;
            height: 100px;
            display: inline-block;
            margin-top: 10px;
            transition: transform 0.3s;
        }

        #contact img:hover {
            transform: scale(1.1);
        }


    </style>
</head>
<body>
    <header class="title-section">
        <h1>Towards Robust Few-shot Class Incremental Learning in Audio Classification
            using Contrastive Representation</h1>
        <div class="authors-info">
            <p class="authors">
                Riyansha Singh<sup>1</sup>, Parinita Nema<sup>2</sup>, Vinod K Kurmi<sup>2</sup>
            </p>
            <p class="affiliations">
                <sup>1</sup>IIT Kanpur, <sup>2</sup>IISER Bhopal 
            </p>
        </div>
    </header>
    
    <nav class="navigation">
        <ul>
            <li><a href="#abstract">Abstract</a></li>
            <li><a href="#download">Resources</a></li>
            <li><a href="#citation">Citation</a></li>
            <li><a href="#contact">Connect</a></li>
        </ul>
    </nav>

    <!-- Headline -->
    <div class="headline">
        Accepted in INTERSPEECH 2024!
    </div>
    
    <main class="container">
        <section id="abstract">
            <!-- <h2>Abstract</h2> -->
            <div class="image-container">
                <a href="images\main.png" target="_blank">
                    <img src="images\main.png" alt= "During base training, we follow a two-stage process. Initially, the model undergoes training employing the contrastive loss
                    function to seperate classes with minimum overlap. Subsequently, the model is trained utilizing the cross-entropy loss function to guide
                    the optimization process towards better classification performance">
                </a>
            </div>
            <p>In machine learning applications, gradual data ingress is common, especially in audio processing where incremental learning is vital for real-time analytics. Few-shot class-incremental
                learning addresses challenges arising from limited incoming data. Existing methods often integrate additional train
                able components or rely on a fixed embedding extractor post training on base sessions to mitigate concerns related
                to catastrophic forgetting and the dangers of model overfitting. However, using cross-entropy loss alone during base session training is suboptimal for audio data. To address
                this, we propose incorporating supervised contrastive learning to refine the representation space, enhancing discriminative power and leading to better generalization since it facilitates seamless integration of incremental classes, upon arrival. Experimental results on NSynth and LibriSpeech datasets
                with 100 classes, as well as ESC dataset with 50 and 10 classes, demonstrate state-of-the-art performance.</p>
        </section>
        
        <div class="side-by-side">
            <section id="download">
                <h2>Download</h2>
                <div class="download-image-container">
                    <a href="https://arxiv.org/abs/2407.19265" download>
                        <img src="images\FsAC_Learning_page-0001.jpg" alt="Download Paper (PDF)">
                    </a>
                </div>
            </section>
            
            <section id="code">
                <h2>Code</h2>
                <a href="https://github.com/riyansha/CIL-RepLearning">
                        <img src="images\github_icon.png" alt="GitHub" class="icon">
                </a>
            </section>

            <section id="dataset">
                <h2>Dataset</h2>
                <ul>
                    <li><a href="https://www.openslr.org/12/">LibriSpeech</a></li>
                    <li><a href="https://magenta.tensorflow.org/datasets/nsynth">NSynth</a></li>
                    <li><a href="https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/YDEPUT">ESC</a></li>
                </ul>
            </section>
        </div>
        <section id ="Citation">
            <h2>Citation</h2>
            <div class="citation-content">
                <p class="citation-note">If you use this work, please cite:</p>
                <pre class="citation-bibtex">
                    @misc{singh2024robustfewshotclassincremental,
                        title={Towards Robust Few-shot Class Incremental Learning in Audio Classification using Contrastive Representation}, 
                        author={Riyansha Singh and Parinita Nema and Vinod K Kurmi},
                        year={2024},
                        eprint={2407.19265},
                        archivePrefix={arXiv},
                        primaryClass={cs.SD},
                        url={https://arxiv.org/abs/2407.19265}, 
                    }
                
                </pre>
            </div>
        </div>
                <section id="contact">
            <h2>Connect</h2>
            <!-- <h2>Contact</h2> -->
            <p>Drop a query at  
                <a href="https://github.com/VisDomLab/FsACLearning/issues">
                    <img src="images\VisDom.png" alt="VisDom" class="icon">
                </a>
           </p>
        </section>
    </main>
</body>
</html>
